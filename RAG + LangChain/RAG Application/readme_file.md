# RAG-based Question Answering System

A complete end-to-end Retrieval-Augmented Generation (RAG) application with FastAPI backend and Streamlit frontend.

## ğŸ—ï¸ Project Structure

```
rag-qa-system/
â”œâ”€â”€ config.py              # Configuration and settings
â”œâ”€â”€ rag_pipeline.py        # RAG pipeline implementation
â”œâ”€â”€ models.py              # Pydantic models
â”œâ”€â”€ main.py                # FastAPI application
â”œâ”€â”€ streamlit_app.py       # Streamlit UI
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env.example          # Environment variables template
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ uploads/              # Directory for uploaded PDFs (auto-created)
â””â”€â”€ vector_db/            # Directory for vector database (auto-created)
```

## ğŸš€ Features

- **PDF Upload & Processing**: Upload single or multiple PDF documents
- **Vector Store Management**: Create, load, and clear vector databases
- **Question Answering**: Ask questions about uploaded documents
- **Source Attribution**: View source documents for each answer
- **Chat History**: Track conversation history
- **RESTful API**: FastAPI backend with automatic documentation
- **Modern UI**: Streamlit interface with real-time updates

## ğŸ“‹ Prerequisites

- Python 3.8+
- Google API Key (for Gemini LLM)

## ğŸ”§ Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd rag-qa-system
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env and add your GOOGLE_API_KEY
```

## ğŸ¯ Usage

### Option 1: Run Both Servers Separately

**Terminal 1 - Start FastAPI backend:**
```bash
python main.py
```
API will be available at: `http://localhost:8000`
API documentation: `http://localhost:8000/docs`

**Terminal 2 - Start Streamlit frontend:**
```bash
streamlit run streamlit_app.py
```
UI will be available at: `http://localhost:8501`

### Option 2: Run with Script

Create a `run.sh` (Linux/Mac) or `run.bat` (Windows) file:

**run.sh:**
```bash
#!/bin/bash
python main.py &
sleep 5
streamlit run streamlit_app.py
```

**run.bat:**
```batch
@echo off
start python main.py
timeout /t 5
streamlit run streamlit_app.py
```

## ğŸ“š API Endpoints

### Health Check
- `GET /` - Root endpoint
- `GET /health` - Health check

### Document Management
- `POST /upload` - Upload single PDF
- `POST /upload-directory` - Process directory of PDFs
- `POST /load-existing-vectors` - Load existing vector store
- `DELETE /clear-vectors` - Clear vector store

### Query
- `POST /query` - Query the RAG system

## ğŸ¨ Streamlit UI Features

### 1. Upload PDF Tab
- Upload PDF documents
- View upload status
- Track processed chunks

### 2. Ask Questions Tab
- Chat interface for Q&A
- Real-time responses
- Source document viewing
- Chat history

### 3. Chat History Tab
- View full conversation
- Export chat as JSON

### Sidebar
- Configuration display
- Vector store management
- Load/Clear operations

## ğŸ”‘ API Usage Examples

### Upload PDF
```python
import requests

files = {"file": open("document.pdf", "rb")}
response = requests.post("http://localhost:8000/upload", files=files)
print(response.json())
```

### Query System
```python
import requests

payload = {
    "question": "What is the main topic?",
    "return_sources": True
}
response = requests.post("http://localhost:8000/query", json=payload)
print(response.json())
```

## âš™ï¸ Configuration

Edit `config.py` or `.env` file to customize:

- **LLM Model**: Change Gemini model version
- **Embedding Model**: Use different Hugging Face model
- **Chunk Size**: Adjust document splitting
- **Retriever K**: Number of documents to retrieve
- **API Ports**: Change server ports

## ğŸ› ï¸ Troubleshooting

### API Not Running
```bash
# Check if port 8000 is in use
lsof -i :8000  # Linux/Mac
netstat -ano | findstr :8000  # Windows
```

### Vector Store Issues
- Clear vector store from UI or API
- Delete `vector_db/` directory manually
- Re-upload documents

### Import Errors
```bash
# Reinstall dependencies
pip install --upgrade -r requirements.txt
```

## ğŸ“ Notes

- First-time embedding model download may take a few minutes
- Large PDFs may take longer to process
- Vector store persists between sessions
- API key is required for LLM functionality

## ğŸ”’ Security

- Never commit `.env` file with real API keys
- Use environment variables for sensitive data
- Implement authentication for production use

## ğŸ“„ License

MIT License

## ğŸ¤ Contributing

Contributions welcome! Please open an issue or submit a PR.

## ğŸ“§ Support

For issues and questions, please open a GitHub issue.