{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai langchain-community langchain-chroma langchain langchain-text-splitters langchain_core langchain_huggingface pypdf"
      ],
      "metadata": {
        "id": "wgrC12a7JVU8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n"
      ],
      "metadata": {
        "id": "VwjUbEpQDXIr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIG FILE\n",
        "# OpenRouter (OpenAI-Router-compatible)\n",
        "BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "OPEN_API_KEY = \"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\"\n",
        "MODEL = \"nvidia/nemotron-nano-9b-v2:free\"\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=BASE_URL,\n",
        "    api_key=OPEN_API_KEY,\n",
        ")\n",
        "\n",
        "# Document path\n",
        "PDF_PATH = \"/content/ML_BOOK.pdf\"\n",
        "\n",
        "# Chunking\n",
        "CHUNK_SIZE = 150\n",
        "CHUNK_OVERLAP = 40\n",
        "\n",
        "# Dense embedding model\n",
        "EMBEDDING_MODEL = \"BAAI/bge-base-en-v1.5\"\n",
        "\n",
        "# Retrieval strategy\n",
        "SEARCH_TYPE = \"hybrid\"\n",
        "TOP_K = 5\n",
        "\n",
        "# Vector DB\n",
        "PERSIST_DIRECTORY = \"/content/chroma_db\"\n"
      ],
      "metadata": {
        "id": "e1HNva9gEvuL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA LOADING\n",
        "\n",
        "def pdf_loading(pdf_path):\n",
        "  loader=PyPDFLoader(pdf_path)\n",
        "  return loader.load()\n",
        "\n"
      ],
      "metadata": {
        "id": "WFna2FAQPddT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHUNKING OF DOCUMENT\n",
        "def text_chunking(documents):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "TYfaQ0nmT_pP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STORE CHUNKS EMBEDDINGS IN VECTORSTORE\n",
        "\n",
        "def store_embedding(chunks):\n",
        "  embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-base-en-v1.5\",\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        "  )\n",
        "\n",
        "  vectorstore=Chroma.from_documents(\n",
        "      documents=chunks,\n",
        "      embedding=embeddings,\n",
        "      persist_directory=PERSIST_DIRECTORY\n",
        "  )\n",
        ""
      ],
      "metadata": {
        "id": "_Ic3cjXXVdq_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ((1))   VECTOR DATA BASE BUILDING FORM RAW PDF TO VECTORSTORE\n",
        "\n",
        "def chroma_db_building(pdf_path):\n",
        "  text=pdf_loading(pdf_path)\n",
        "  chunks=text_chunking(text)\n",
        "  store_embedding(chunks)"
      ],
      "metadata": {
        "id": "44ono2rLWnyY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN FILE FOR DATABASE BUILDING\n",
        "chroma_db_building(PDF_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58nBMIRCXkX_",
        "outputId": "417157cb-2624-4eef-8a4e-3cd9b9d6c6af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize embeddings ONCE (outside the function)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-base-en-v1.5\",\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Load existing Chroma vector store\n",
        "vectorstore = Chroma(\n",
        "    persist_directory=PERSIST_DIRECTORY,\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "\n",
        "# RETRIEVAL\n",
        "def retrieve_context(query, k=TOP_K):\n",
        "    results = vectorstore.similarity_search(query, k=k)\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in results)\n",
        "    return context\n"
      ],
      "metadata": {
        "id": "dwTsWEw6aZ1m"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_chat(user_query, conversation_history=[]):\n",
        "    \"\"\"RAG-based chat function.\"\"\"\n",
        "    # Retrieve relevant context\n",
        "    context = retrieve_context(user_query)\n",
        "\n",
        "    # Create prompt with context\n",
        "    system_message = f\"\"\"You are a helpful assistant for a company.\n",
        "Use the following context to answer the user's question.\n",
        "If the answer is not in the context, say so politely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "    # Prepare messages\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    messages.extend(conversation_history)\n",
        "    messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "    # Get response from OpenRouter\n",
        "    completion = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        extra_headers={\n",
        "            \"HTTP-Referer\": \"https://company-chatbot.local\",\n",
        "            \"X-Title\": \"Company RAG Chatbot\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    response = completion.choices[0].message.content\n",
        "    return response, context"
      ],
      "metadata": {
        "id": "cNZPmE3jcCwu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_chatbot():\n",
        "    \"\"\"Run the interactive RAG chatbot.\"\"\"\n",
        "    print(\"Company RAG Chatbot Started!\")\n",
        "    print(\"Type 'quit' to exit, 'context' to see retrieved context\\n\")\n",
        "\n",
        "    conversation_history = []\n",
        "    last_context = \"\"\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if user_input.lower() == 'context':\n",
        "            print(f\"\\nLast Retrieved Context:\\n{last_context}\\n\")\n",
        "            continue\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            response, context = rag_chat(user_input, conversation_history)\n",
        "            last_context = context\n",
        "\n",
        "            print(f\"\\nBot: {response}\\n\")\n",
        "\n",
        "            # Update conversation history\n",
        "            conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "            conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "            # Keep only last 10 messages to manage context window\n",
        "            if len(conversation_history) > 10:\n",
        "                conversation_history = conversation_history[-10:]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\\n\")\n",
        "\n",
        "# Run the chatbot\n",
        "run_chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxSCR7fDcCvY",
        "outputId": "72e9d277-9d07-4c95-b91b-afd34d923b84"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Company RAG Chatbot Started!\n",
            "Type 'quit' to exit, 'context' to see retrieved context\n",
            "\n",
            "You: hi\n",
            "\n",
            "Bot: Hello! How can I assist you today? ðŸ˜Š\n",
            "\n",
            "\n",
            "You: i need to know about supervised learning \n",
            "\n",
            "Bot: Supervised learning is a type of machine learning where the algorithm learns from a labeled dataset, meaning each training example is paired with an output label. The goal is for the model to learn the mapping from inputs to correct outputs, so it can make accurate predictions on new, unseen data. Key steps include gathering labeled data, training a model (often model-based), and using this model to generalize patterns from the examples it was trained on. This contrasts with unsupervised learning, which uses unlabeled data to find hidden structures.\n",
            "\n",
            "\n",
            "You: what is decision try give me information in bullet points \n",
            "\n",
            "Bot: - A decision tree is an **acyclic graph** (directed acyclic graph) used for decision-making.  \n",
            "- It makes decisions by **splitting data** at each node based on feature values.  \n",
            "- It **implicitly or explicitly defines decision boundaries** to separate classes or predict outcomes.  \n",
            "- The tree stops growing when a **predefined stopping criterion** (e.g., maximum depth or number of nodes) is met.  \n",
            "- Decision trees are **explainable models**, making it easier to interpret decisions.  \n",
            "- They **shortlist algorithms** by trying different splits (e.g., features, thresholds) to find optimal paths.  \n",
            "- Used in **supervised learning** to build models that predict target values for new examples.\n",
            "\n",
            "\n",
            "You: quit\n",
            "Goodbye!\n"
          ]
        }
      ]
    }
  ]
}